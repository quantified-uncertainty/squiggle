import { Model } from "@/model/utils";
import { Catalog, InterfaceWithModels, Item } from "@/types";
import { Map } from "immutable";

// Note: Much of this was generated by this file:
// https://observablehq.com/d/98cbe0073226a5f9

function getCatalog(): Catalog {
  const items: Item[] = [
    {
      id: "i0",
      name: "Fish Welfare Initiative's value so far",
      clusterId: "papers",
    },
    {
        id: "i1",
        name: "Value of Wild Animal Initiative's work in 2023",
        clusterId: "papers",
      },
      {
        id: "i2",
        name: "Value of Beyond Meat, in terms of expected farmed cow lives averted in 2023",
        clusterId: "papers",
      },
      {
        id: "i3",
        name: "Value of the Against Malaria Foundation, in 2021",
        clusterId: "papers",
      },
      {
        id: "i4",
        name: "Value of avoiding medium-sized asteroid impacts for a year",
        clusterId: "papers",
      },
      {
        id: "i5",
        name: "Value of a marginal QURI ops hour",
        clusterId: "papers",
      },
      {
        id: "i6",
        name: "Value of a marginal hour of undirected research at QURI",
        clusterId: "papers",
      },
      {
        id: "i7",
        name: "Value of an hour of doing research management QURI",
        clusterId: "papers",
      },
      {
        id: "i8",
        name: "Value of an hour of doing research programming at QURI",
        clusterId: "papers",
      },

      {
        id: "i9",
        name: "Value of an hour of doing median research programming at QURI",
        clusterId: "papers",
      },
  ];

  return {
    id: "cross-cause",
    title: "Cross-Cause Values",
    items,
    clusters: {
      papers: {
        name: "Papers",
        color: "#DB828C",
      },
    },
  };
}

function getTextModel(): Model {
  return {
    title: "Cross Cause Estimate",
    author: "Nuno Sempere",
    mode: "text",
    code: `
    /* Combined model

    Purpose: Give relative values for everything in a list of 10-20 elements
    
    In particular, give expected(A)/expected(B), and then add one order of magnitude
    to either side. 
    
    */
    
    // Utils
    d_0 = mixture(0)
    
    // # GENERAL FACTORS
    
    moral_circle = {
      speculative_longtermism: 6, // quri research hours?
      human_centric_consequentialism: 6, // generic human qalys
      large_animal_inclusive_consequentialism: 1, // cow / pig qalys
      small_animal_inclusive_consequentialism: 0.2 // fish qalys
    }
    
    value_of_fish_qalys_relative_to_human_qalys_under_small_animal_inclusive_consequentialism = beta(0.8277362357555023, 25.259989675532076)
    // ^ 0.001 to 0.1, from <https://nunosempere.com/blog/2023/03/15/fit-beta/>
    value_of_shrimp_qalys_relative_to_human_qalys_under_small_animal_inclusive_consequentialism = value_of_fish_qalys_relative_to_human_qalys_under_small_animal_inclusive_consequentialism
    // ^ seems funny to consider shrimp as valuable as fish
    // because I feel more warmly towards fish
    // But this isn't an overriding consideration, 
    // because they seem similarly complex animals
    value_of_small_animal_qalys_relative_to_human_qalys_under_small_animal_inclusive_consequentialism = value_of_fish_qalys_relative_to_human_qalys_under_small_animal_inclusive_consequentialism
    // taking fish as a representative of "small animal"
    
    // # INTERVENTIONS BY CLUSTER 
    
    // ## Cluster 1: Animal charities
    
    // Intervention 1.1: Fish Welfare Initiative's work so far
    /* Sources
    - https://www.fishwelfareinitiative.org/impact
    - https://forum.effectivealtruism.org/posts/T5fSphiK6sQ6hyptX/opinion-estimating-invertebrate-sentience#Peter_Hurford
    - https://forum.effectivealtruism.org/posts/Qk3hd6PrFManj8K6o/rethink-priorities-welfare-range-estimates
    - https://nunosempere.com/blog/2023/02/19/bayesian-adjustment-to-rethink-priorities-welfare-range-estimates/
    */
    
    number_of_fish_potentially_helped_by_fwi = 1M to 2M
    number_of_shrimp_potentially_helped_by_fwi = 1M to 2M
    // ^ [animal] potentially helped 
    // is a measure from fwi's own assessment 
    // (<https://www.fishwelfareinitiative.org/impact>)
    
    timespan_fish_helped = 0.5 to 2 
    proportion_qaly_improvement = beta(1.9894047192566546, 6.370718188981973)
    // ^ 90% confidence interval is 0.05 to 0.5, from <https://nunosempere.com/blog/2023/03/15/fit-beta/>.
    // I'm just really unsure ¯\_(ツ)_/¯
    
    num_small_animal_qalys_fwi = timespan_fish_helped * (number_of_fish_potentially_helped_by_fwi  + number_of_shrimp_potentially_helped_by_fwi) * proportion_qaly_improvement
    
    item_fwi = {
      cluster: "1",
      id: "1.1",
      short_name: "FWI value so far",
      name: "Fish Welfare Initiative's value so far",
      description: "Value of the Fish Welfare Initiative's work so far as outlined in <https://www.fishwelfareinitiative.org/impact> as of 21/03/2023 (<https://web.archive.org/web/20230315011504/https://www.fishwelfareinitiative.org/impact>). The reasoning for not disaggregating by year is that their impact page (<https://www.fishwelfareinitiative.org/impact>) doesn't.",
      value: {
          quri_research: d_0,
          human_qalys: d_0,
          large_animal_qalys: d_0,
          small_animal_qalys: num_small_animal_qalys_fwi
      }
    }
    
    // Intervention 1.2: Wild Animal Initiative 
    
    /* Estimation strategy:
    - For now, I am choosing to model its impact as a chance that it influences further donations. Key parameters will be: how much funding has it influenced so far, what the value of that funding it.
    - Note that my impression is that WAI has funded _research_, but not things which ultimately cash out in improved animal welfare, which is what I am estimating
    - Note also that this is a bit underdefined, i.e., I'm not sure whether I'm talking about funding influenced so far or about funding which will be influenced.
    */
    
    /* Sources
    - https://www.openphilanthropy.org/grants/wild-animal-initiative-animal-welfare-research/
    - https://www.wildanimalinitiative.org/
    - https://www.wildanimalinitiative.org/services
    */
    
    funding_moved = mixture(
      [0, truncate(1M to 10M, 0, 100M), truncate(5M to 200M, 0, 1B)], [0.5, 0.4, 0.1]
      )
    share_of_funding_to_large_animals = SampleSet.fromDist(beta(3.261811017550073, 3.2618060609030035))
    // ^ beta whose 90% confidence interval is 0.2 to 0.8, from <https://nunosempere.com/blog/2023/03/15/fit-beta/>
    // I'm just very unsure
    // The sampleset is so that we can preserve correlation, i.e., so that
    // share_of_funding_to_large_animals + share_of_funding_to_small_animals = 1 (!) :^)
    share_of_funding_to_small_animals = 1 - share_of_funding_to_large_animals
    // you can see that 
    // share_of_funding_to_large_animals + share_of_funding_to_small_animals
    // is equal to 1, in Squiggle, just paste the preceding lines into 
    // https://www.squiggle-language.com/playground
    // :^)
    
    large_animal_advantage = truncate(0.1 to 1k, 0, 1M) 
    // ^ much easier to buy large animal qalys than human qalys :^)
    small_animal_advantage = truncate(100 to 10k, 0, 1M) 
    // ^ way easier to buy large animal qalys than human qalys :^)
    
    value_of_wai_for_large_animals = funding_moved * share_of_funding_to_large_animals * large_animal_advantage
    value_of_wai_for_small_animals = funding_moved * share_of_funding_to_small_animals * small_animal_advantage
    // ^ note that the unit for these *is* animal qalys
    // funding * human qaly cost effectiveness of AMF * how much easier it is to buy animal over human qalys
    // :^)
    
    item_wai = {
      cluster: "1",
      id: "1.2",
        short_name: "WAI 2023",
      name: "Value of Wild Animal Initiative's work in 2023",
      description: "Something like, ~expected value of the Wild Animal Inititative's Work so far? This estimation is a bit under-defined.",
      value: {
          quri_research: d_0,
          human_qalys: d_0,
          large_animal_qalys: value_of_wai_for_large_animals,
          small_animal_qalys: value_of_wai_for_small_animals
      }
    }
    
    // Intervention 1.3: Beyond Meat
    
    /* Sources 
    - https://apnews.com/article/mcdonalds-corp-europe-business-a5b9d53c77f6df945fd2ecb9a18ca1a1
      - "On Thursday, Walmart was advertising Beyond Meat burgers at $9.68 per pound; lean ground beef was $5.86 per pound."
      - Beyond Meat said it expects net revenue in the range of $375 million to $415 million this year, which would be lower than the $418 million in reported in 2022. 
    - https://faunafacts.com/cows/how-many-burgers-in-one-cow/
    */
    
    beyond_meat_price_per_pound = 5 to 12
    beyond_meat_revenues = 375M to 415M
    beyond_meat_pounds_sold = beyond_meat_revenues / beyond_meat_price_per_pound
    
    quarter_pounders_per_cow = 1000 to 1600
    pounds_of_meat_per_cow = quarter_pounders_per_cow / 4
    cows_saved_if_all_meat_was_counterfactual = beyond_meat_pounds_sold / pounds_of_meat_per_cow
    
    chance_purchase_was_counterfactual = beta(4.054018273265266, 25.75259685760455)
    // ^ confidence interval is 0.05 to 0.25, from <https://nunosempere.com/blog/2023/03/15/fit-beta/>
    // I imagine that most people would have chosen a vegetarian alternative
    // but also that having tasty meat replacements makes it easier to
    // keep being a vegetarian.
    // Very uncertain, though
    lifespan_beef_cow = (30 to 42) / 12
    
    value_of_beyond_meat_in_large_animal_qalys =  cows_saved_if_all_meat_was_counterfactual * chance_purchase_was_counterfactual * lifespan_beef_cow
    
    item_beyond_meat = {
      cluster: "1",
      id: "1.3",
      short_name: "Beyond Meat 2023",
      name: "Value of Beyond Meat, in terms of expected farmed cow lives averted in 2023",
      description: "Value of Beyond Meat, in terms of expected farmed cow lives averted in 2023. For simplicity, ignores consumer surplus, industry mimesis effects, etc.",
      value: {
          quri_research: d_0,
          human_qalys: d_0,
          large_animal_qalys: value_of_beyond_meat_in_large_animal_qalys,
          small_animal_qalys: d_0
      }
    }
    
    /* ## Cluster 2: Diverse interventions of different types */
    
    // Intervention 2.1: Against Malaria Foundations
     amf_baseline_cost_effectiveness_per_dollar = 0.011 to 0.0821 
     // ^source:  <https://observablehq.com/d/43b277c2743896a6> (<https://observablehq.com/@tanaerao/malaria-nets-analysis>)
     // units here are, very roughly, similar to qalys
     cost_effectiveness_per_dollar_with_counterfactual_adjustment = mixture(
      [
        0, 
        amf_baseline_cost_effectiveness_per_dollar,
        -amf_baseline_cost_effectiveness_per_dollar
      ],
      [0.1, 0.8, 0.1]
     )
    budget_amf_2021 = 92M
    // ^ From its Form 990 
    // (<https://www.causeiq.com/organizations/view_990/203069841/18a54235eb7569bea4f7440340e1f3bf>)
    value_amf_2021 = cost_effectiveness_per_dollar_with_counterfactual_adjustment * budget_amf_2021
    
    item_against_malaria_foundation = {
      cluster: "2",
      id: "2.1",
      short_name: "AMF 2021",
      name: "Value of the Against Malaria Foundation, in 2021",
      description: "Value of the Against Malaria Foundation, in 2021. For simplification purposes, I am choosing to interpret GiveWell's 'units of value' as QALYs, even though this isn't quite the case.",
      value: {
          quri_research: d_0,
          human_qalys: value_amf_2021,
          large_animal_qalys: d_0,
          small_animal_qalys: d_0
      }
    }
    
    // Intervention 2.2: Value of avoiding at least medium-sized asteroid impacts for a year 
    /*
    Sources: 
    - https://forum.effectivealtruism.org/posts/mL2t47FzGqcsPXv6i/shallow-report-on-asteroids
    - For the population pyramid: <https://ourworldindata.org/global-population-pyramid>
    */
    
    tunguska_like_asteroid_event_loss =  1.94 * 10e7 // DALYs.
    tunguska_yearly_probability = 0.5/100
    
    large_sub_global_asteroid_event_loss = 2.76 * 10e8 
    large_probability = 0.004/100
    
    global_asteroid_event_loss =  4.55 * 10e11 
    global_probability = 0.0002/100
    
    rare_k_t_scale_asteroid_event_loss = 8.42 * 10e11 
    rare_probability= 0.000001/100
    
    p_no_large_asteroids = 1 - (tunguska_yearly_probability + large_probability + global_probability + rare_probability)
    
    qaly_loss_per_death = truncate(5 to 60, 0, 120)
    // fitting a population pyramid to a lognormal. 
    // very unprincipled :( 
    // This also comes up in some estimates of AMF, 
    // where they don't disaggregate under 5 years olds by age 
    
    value_no_large_asteroids_year = mixture(
      [tunguska_like_asteroid_event_loss, large_sub_global_asteroid_event_loss, global_asteroid_event_loss, rare_k_t_scale_asteroid_event_loss, 0],
      [tunguska_yearly_probability, large_probability, global_probability, rare_probability, p_no_large_asteroids]
    ) * qaly_loss_per_death
    
    item_no_asteroids = {
      cluster: "2",
      id: "2.2",
      short_name: "No big asteroids 2023",
      name: "Value of avoiding medium-sized asteroid impacts for a year",
      description: "Cribbing from this great post: https://forum.effectivealtruism.org/posts/mL2t47FzGqcsPXv6i/shallow-report-on-asteroids.",
      value: {
          quri_research: d_0,
          human_qalys: value_no_large_asteroids_year,
          large_animal_qalys: d_0, // actually it would also kill large numbers of animals, but ignoring that.
          small_animal_qalys: d_0
      }
    }
    
    /* Cluster 3: Various research outputs at QURI */
    
    /*
    Value of various activities Ozzie can be doing
    
    The way to model this might be:
    - QURI is a black box which produces research outputs
    - Ops are necessary to keep the black box running, but it's unclear how many hours.
    
    Things to compare:
    
    - Value of 1h of marginal ops (Nics/Maria, but saying "ops" so that it is readable by people without much context)
    - Value of 1h of Ozzie's freeform research and theorizing
    - Value of 1h of Nuño's time researching
    - Value of 1h of Slava's time programming
    - [ Not done ] 1h of Ozzie managing Slava
    - 1h of Ozzie managing Nuño's research
    
    Ignoring:
    - FB
    - RP board member
    Because the comparison wouldn't be as neat
    
    */
    
    /* Managing operations 
    The way I am choosing to model this is that there is a number of hours of necessary operations, without which QURI breaks,
    and hours beyond that are not necessary.
    
    This is very simplistic, because it doesn't take into account how ops can do things other than make QURI not break,
    e.g., organize contests, help Slava get a visa, make research go faster, etc.
    */
    
    // Item 3.1: A marginal hour of ops research
    
    p_marginal_ops_hour_is_necessary = 0.3 // uninformed guess
    unlocked_hours_if_necessary = 1 to 10
    value_ops_quri = mixture([0, unlocked_hours_if_necessary], [1-p_marginal_ops_hour_is_necessary, p_marginal_ops_hour_is_necessary]) 
    
    item_marginal_quri_ops = {
      cluster: "3",
      id: "3.1",
      short_name: "Marginal ops hour @ QURI",
      name: "Value of a marginal QURI ops hour",
      description: "Value of a marginal hour of operations at QURI, in terms of keeping QURI running (so ignoring value of making activities/hiring/visas/... for now). Units are: median hour of QURI research",
      value: {
          quri_research: value_ops_quri,
          human_qalys: d_0, // note that hopefully it eventually does contribute to human QALYs, it's just that these aren't estimated directly
          large_animal_qalys: d_0, 
          small_animal_qalys: d_0
      }
    }
    
    /* Freeform research */
    value_freeform_research_quri = mixture([0, 0.5 to 20], [0.2, 0.8])
    // compared to an "average" hour of QURI's research
    // source: vibes
    item_freeform_research_quri = {
      cluster: "3",
      id: "3.2",
      short_name: "1h Freeform Research @ QURI",
      name: "Value of a marginal hour of undirected research at QURI",
      description: "Value of a marginal hour of undirected research at QURI. Subjective estimate, with reference to the median hour of research at QURI",
      value: {
          quri_research: value_freeform_research_quri,
          human_qalys: d_0, // note that hopefully it eventually does contribute to human QALYs, it's just that these aren't estimated directly
          large_animal_qalys: d_0, 
          small_animal_qalys: d_0
      }
    }
    
    /* Managing Nuño's research */
    value_research_management_quri = mixture([0, 1 to 10], [0.4, 0.6])
    // sometimes these are basically useless, 
    // but sometimes they are very useful
    item_research_management_quri = {
      cluster: "3",
      id: "3.3",
      short_name: "1h Research Management @ QURI",
      name: "Value of an hour of doing research management QURI",
      description: "Value of a typical hour of research management at QURI, e.g., a weekly meeting. Subjective estimate, with reference to the median hour of research at QURI",
      value: {
          quri_research: value_research_management_quri,
          human_qalys: d_0,
          large_animal_qalys: d_0, 
          small_animal_qalys: d_0
      }
    }
    
    /* Value of 1h of Slava's time programming */
    value_quri_programming_quri = 0.5 to 5
    item_quri_programming = {
      cluster: "3",
      id: "3.3",
      short_name: "1h Programming @ QURI",
      name: "Value of an hour of programming at QURI",
      description: "Value of a typical hour of programming time at QURI, e.g., creating a new tool, improving or mantaining Squiggle/Metaforecast/Guesstimate",
      value: {
          quri_research: value_quri_programming_quri,
          human_qalys: d_0,
          large_animal_qalys: d_0, 
          small_animal_qalys: d_0
      }
    }
    
    /* Value of the median 1h of my (Nuño's) time researching */
    value_median_quri_research_hour = 1 // baseline unit
    // note that I think that the value of the median hour of my time is much worse
    // than the expected value of a research hour, since sometimes I am inspired
    // and I produce fairly valuable stuff
    
    value_median_quri_research = 0.5 to 5
    item_median_quri_research = {
      cluster: "3",
      id: "3.3",
      short_name: "1h Programming @ QURI",
      name: "Value of an hour of programming at QURI",
      description: "Value of a typical hour of programming time at QURI, e.g., creating a new tool, improving or mantaining Squiggle/Metaforecast/Guesstimate",
      value: {
          quri_research: value_median_quri_research,
          human_qalys: d_0,
          large_animal_qalys: d_0, 
          small_animal_qalys: d_0
      }
    }
    
    // # Conversion factors
    
    /*
    
    - Small animal qaly → large animal qaly
    - Large animal qaly → Human qaly
    - QURI research <-> human qaly: This one is tricky
    
    */
    
    small_animal_to_large_animal = beta(1.1049756286343138, 14.271888425116993)
    // fitting a beta to have a 90% confidence interval of 0.005 to 0.2
    // essentially very arbitrary, based on subjective estimates
    large_animal_to_human = beta(1.9894047192566546, 6.370718188981973)
    // fitting a beta to have a 90% confidence interval of 0.05 to 0.5
    
    /* QURI research in terms of human QALYs: man this is tricky
    
    One way to think about QURI's impact is that our research on estimation will improve 
    the prioritization of the EA community.
    
    This can be decomposed into:
    
    - Capital prioritization: E.g., influencing EA Funds, Open Philanthropy, etc. to 
      give out their capital better
    - Labour prioritization: Influencing individual EAs to make decisions different.
    
    The problem with this is that our influence right now seems a bit small. Most of 
    our possible value is future expected value.
    
    */
    
    capital_who_will_listen_to_us = mixture([0, truncate(1M to 10M, 0, 100M), truncate(5M to 500M, 0, 20B)], [0.3, 0.5, 0.2])
    improvement_over_that_capital = beta(1.04786893882528, 60.175072401582355)
    // ^ 0.1% to 5%
    // I do think that you can get improvements of 5% essentially
    // by dropping the worldview diversification approach
    // though I think it's very unlikely that I could 
    // convince Open Philanthropy of that.
    
    global_health_advantage = 1 // nothing, AMF is the baseline
    longtermism_advantage = truncate(10k to 100k, 0, 100M) 
    initial_value_of_capital = amf_baseline_cost_effectiveness_per_dollar * mixture(
      [global_health_advantage, (large_animal_advantage + small_animal_advantage)/2, longtermism_advantage], 
      [ 1, 1, 1]
    )
    value_quri_as_influence_on_capital = capital_who_will_listen_to_us * 
      improvement_over_that_capital * 
      initial_value_of_capital
    
    number_of_people_who_will_meaningfully_make_different_decisions = mixture([0, truncate(1 to 100, 0, 1k), truncate(50 to 1k, 0, 20k)], [0.1, 0.7, 0.2])
    monetary_value_of_decision_improvement = mixture([0, truncate(1k to 10k, 0, 100k), truncate(100k to 100M, 0, 1B)], [0.5, 0.4, 0.1])
    value_of_quri_from_decision_improvements = monetary_value_of_decision_improvement * mixture(
      [global_health_advantage, (large_animal_advantage + small_animal_advantage)/2, longtermism_advantage], 
      [ 1, 1, 1]
    )
    
    value_of_quri = value_quri_as_influence_on_capital + value_of_quri_from_decision_improvements
    // most of this is going to come from the "longtermism advantage" part.
    // it's also going to contain animal-qalys,
    // but say that this is mostly human-qaly-equivalents
    value_of_quri_in_human_qalys = value_of_quri
    
    // but this is over a period of, say, three years, and we want the value
    // of a median hour.
    num_employees = 3 to 6
    hours_in_three_years =  (num_employees * 3 * 50 * 50) 
    // ^ 3 years, 50 weeks a year, 50 hours employee/week
    value_per_quri_hour_in_qalys = value_of_quri_in_human_qalys / hours_in_three_years
    
    // value_per_quri_hour_in_qalys
    
    /* Comparison function */
    getCommonUnit = {|levels_i| 
      common_unit_is_human_qalys = 
        (levels_i[0] == true) ||
        (levels_i[1] == true && levels_i[2] == true) ||
        (levels_i[1] == true && levels_i[3] == true) 
      
      common_unit_is_quri_research = !(common_unit_is_human_qalys) &&
        (levels_i[1] == true )
      
      common_unit_is_large_animals = !(common_unit_is_human_qalys) && 
        !(common_unit_is_quri_research) &&
        (levels_i[2] == true)
      
      common_unit_is_small_animals = !(common_unit_is_human_qalys) && 
        !(common_unit_is_quri_research) && 
        !(common_unit_is_large_animals) &&
        (levels_i[3] == true)
      
      no_common_unit = !common_unit_is_large_animals
      
      result = common_unit_is_human_qalys ? 1 : 
        (common_unit_is_quri_research ? 2 : (
          common_unit_is_large_animals ? 3 : (
            common_unit_is_small_animals ? 4 : 5
          )
        )
        )
      1
    }
    
    isZero = {|x| // a hack, for now
      result = mean(mixture(x)) == 0
      result 
    }
    
    getRelativeValues = { |intervention_1, intervention_2|
      values_1 = intervention_1.value
      values_2 = intervention_2.value
      levels_1 = [ isZero(values_1.human_qalys), isZero(values_1.quri_research), isZero(values_1.large_animal_qalys), isZero(values_1.small_animal_qalys) ]
    
      levels_2 = [ isZero(values_2.human_qalys), isZero(values_2.quri_research), isZero(values_2.large_animal_qalys), isZero(values_2.small_animal_qalys)]
    
      levels_common = [
        levels_1[0] || levels_2[0],
        levels_1[1] || levels_2[1],
        levels_1[2] || levels_2[2],
        levels_1[3] || levels_2[3]
      ]
    
      common_unit = getCommonUnit(levels_common)
    
      if common_unit == 1 then { // human qalys
    
        human_value_1 = values_1.human_qalys * 1 + 
          values_1.quri_research * value_per_quri_hour_in_qalys +
          values_1.large_animal_qalys * large_animal_to_human +
          values_1.small_animal_qalys *  small_animal_to_large_animal * large_animal_to_human 
        
        human_value_2 = values_2.human_qalys * 1 + 
          values_2.quri_research * value_per_quri_hour_in_qalys +
          values_2.large_animal_qalys * large_animal_to_human +
          values_2.small_animal_qalys *  small_animal_to_large_animal * large_animal_to_human 
        
        relative_value_point_estimate = mean(human_value_1)/mean(human_value_2)
        relative_value_range = to(relative_value_point_estimate/10, relative_value_point_estimate*10)
        relative_value_range
    
      } else {
        (if common_unit == 2 then { // quri research
    
          quri_research_value_1 = values_1.quri_research 
          quri_research_value_2 = values_2.quri_research
          relative_value_point_estimate = mean(quri_research_value_1)/mean(quri_research_value_1)
          relative_value_range = to(relative_value_point_estimate/10, relative_value_point_estimate*10)
          relative_value_range
    
        } else {
          (if common_unit == 3 then { // large animal
    
              large_animal_value_1 = 
                values_1.large_animal_qalys * large_animal_to_large_animal +
                values_1.small_animal_qalys *  small_animal_to_large_animal 
              
              large_animal_value_2 = 
                values_2.large_animal_qalys * large_animal_to_large_animal +
                values_2.small_animal_qalys *  small_animal_to_large_animal 
              
              relative_value_point_estimate = mean(large_animal_value_1)/mean(large_animal_value_2)
              relative_value_range = to(relative_value_point_estimate/10, relative_value_point_estimate*10)
              relative_value_range
    
          } else {
            if common_unit == 4 then {
              small_animal_value_1 = values_1.small_animal_qalys
              small_animal_value_2 = values_2.small_animal_qalys
              relative_value_point_estimate = mean(small_animal_value_1)/mean(small_animal_value_2)
              relative_value_range = to(relative_value_point_estimate/10, relative_value_point_estimate*10)
              relative_value_range
              
            } else {
              - 42 // some value to indicate that things have gone wrong 
            }
          })
        } )
      }
      
    }
    
     /* Final presentation */
    
     items = [
      item_fwi, item_wai, item_beyond_meat, 
      item_against_malaria_foundation, item_no_asteroids,
      item_marginal_quri_ops, item_freeform_research_quri, item_research_management_quri, item_quri_programming, item_median_quri_research
     ]
    
    // List.map(items, {|x| x.name})
    
    /*
    
    
    
    */
    
    //  item_A = items[7]
    //  item_B = items[8]
    
    // {
    //   item_A: item_A,
    //   item_B: item_B,
    //   relativeValue: getRelativeValues(item_A, item_B)
    // }
    

    foo = {"i0": item_fwi, "i1": item_wai, "i2": item_beyond_meat, "i3": item_against_malaria_foundation, "i4": item_no_asteroids, "i5": item_marginal_quri_ops, "i6":item_freeform_research_quri, "i7": item_research_management_quri, "i8": item_quri_programming, "i9":item_median_quri_research }
    fn(a,b) = getRelativeValues(foo[a], foo[b])
    fn
`,
  };
}

export function getCrossCauses(): InterfaceWithModels {
  return {
    catalog: getCatalog(),
    models: Map([["Cross-Cause-Estimate", getTextModel()]]),
  };
}

[
  {
    "id": "submission-denkenberger-1740678284167",
    "author": "Denkenberger",
    "text": "A few of us at ALLFED (myself, @jamesmulhall, and others) have been thinking about response planning for essential (vital) workers in extreme pandemics. Our impression is that there's a reasonable chance we will not be prepared for an extreme pandemic if it happens, so we should have back-up plans in place to keep basic services functioning and prevent collapse. We think this is probably a neglected area that more people should be working on, and we're interested in whether others think this is likely to be a high-impact topic. We decided to compare it to a standard and evidence-backed intervention to protect the vital workforce that is receiving funding from EA — stockpiling of pandemic proof PPE (P4E).\n\nWe asked Squiggle AI to create two cost-effectiveness analyses comparing stockpiling P4E vs research and planning to rapidly scale up after the outbreak transmission-reducing interventions (e.g. UV) to keep essential workers safe. Given the additional costs of both interventions could be significantly lowered by influencing funding governments have already allocated to stockpiling/response planning, we ran the model with (linked here) and without a message (linked here) to only consider the costs of philanthropic funding.\n\nSummary result:\n\n    Considering all spending, research and planning is estimated as 34 (8.5–140) times as cost-effective as stockpiling\n    Considering only philanthropic spending, research and planning is estimated as 47 (23–100) times as cost-effective as stockpiling\n    We did not feed any numbers into the model, but the ones it self generated seemed reasonably sensible (e.g., Kevin Esvelt's quote of $20 billion for stockpiling adequate PPE for the US falls within the $4-20 billion estimate by the model)\n\nPrompt:\n\nCreate a cost-effectiveness analysis comparing two interventions to keep US essential workers safe in a pandemic with extremely high transmissibility and fatality rates. Assess the interventions on the probability they are successful at preventing the collapse of civilization. Only include money spent before the pandemic happens as there will be plenty of money available for implementation after it starts.\n\n1: Stockpiling elastomeric half mask respirators and PAPRs before the extreme pandemic.\n\n2: Researching and planning to scale up transmission reduction interventions rapidly after the pandemic starts, including workplace adaptations, indoor air quality interventions (germicidal UV, in-room filtration, ventilation), isolation of workers in on-site housing, and contingency measures for providing basic needs if infrastructure fails.\n\nOutputs:\n\n- narrative and explanations of the logic behind all of the numbers used\n\n- ranges of costs for the two options\n\n- ranges of effectiveness for the two options\n\n- cost-effectiveness for the two options\n\n- mean and median ratios of cost effectiveness of planning vs stockpiling\n\n- distribution plots of the cost effectiveness of planning vs stockpiling\n\nOptional message:\n\nImportant: only account for philanthropic funding costs to make these interventions happen. Assume that governments already have pandemic preparedness funding allocated for stockpiles and response planning. This may reduce philanthropic costs if stockpiling interventions can redirect government purchases from disposable masks to more protective elastomeric respirators/PAPRs or if research and planning interventions can add their recommendations to existing government frameworks to prepare essential industries for disasters.\n"
  },
  {
    "id": "submission-shankar-sivarajan-1740678382852",
    "author": "Shankar Sivarajan",
    "text": "Summary: The effort required to manually do the calculations an LLM does to answer a simple query (in Chinese, for the Searle's Room reference) is about what it'd take to build a modern million-man city from scratch.\n\nModel: \n\nSay a human can perform 1 multiply-accumulate (MAC) operation every 5 seconds.\n\nFirst, we produce an estimate for single token generation for Llama 3 8B: 8 billion parameters, about 2 MAC operations per parameter, and with some additional overhead for attention mechanisms, feedforward layers, and other computations, estimate 50 billion MAC operations per token.\n\nThat's 250×109 seconds/token ≈ 70×106 hours.\n\nEstimate full-time work for a year is 8 hours/day, 5 days/week, 50 weeks/year ≈ 2000 hours/year.\n\n70×106 hours ÷ 2,000 hours/man-year ≈ 35,000 man-years/token.\n\nTokens in a simple Chinese question + answer pair: \n\nQuestion: ~5–10 tokens; Answer: ~10–30 tokens; Total: ~15–40 tokens.\n\nSo in total, about 500,000–1,500,000 man-years.\n\n \n\nFor building a city, the most important factors are \n\nInfrastructure Construction (3–5 years):\n\n    Roads, bridges, and transportation networks.\n    Water supply systems (reservoirs, pipelines, treatment plants).\n    Sewage and waste management systems.\n    Electrical grids, telecommunications, and internet infrastructure.\n\nLabor: ~10,000 workers.\n\nMan-years: 30,000–50,000 man-years.\n\nResidential and Commercial Buildings (5–10 years):\n\n    Construction of housing for ~1 million people (apartments, single-family homes).\n    Building commercial spaces (offices, shops, markets).\n    Interior finishing and utilities installation.\n\nLabor: ~20,000 workers.\n\nMan-years: 100,000–200,000 man-years.\n\nIncluding planning and design, site preparation (clearing land, building access road, and excavation for foundations), estimate about 150,000–300,000 man-years depending on the size.\n\nValidating this estimate, the city of Brasília, built in the 1950s, took about 5 years to construct a city for ~500,000 people, involving ~60,000 workers, which translates to ~300,000 man-years.\n\nAssuming it scales proportionally with population, manually performing the calculations to answer a simple Chinese query is about as hard as building a city with 1–2 million population.\n\nTechnique: DeepSeek, but I cut down its verbose answers.\n"
  },
  {
    "id": "submission-niplav-1740678478011",
    "author": "niplav",
    "text": "Model in Squiggle code:\n\n```\n/* Trying to estimate how much AI research has happened so far */\n\ndartmouth_attendees=10 to 12 /* Number of Dartmouth attendees */\nluminaries = (4 to 5)*dartmouth_attendees\nstart_year = 1956 /* Darthmouth */\nend_year = 2025 /* now*/\ndomain = [start_year, end_year]\ngrowth_rate=1.1 to 1.15\n\nexport ai_researchers(t)={luminaries*growth_rate^(normal(t-start_year, (t-start_year+1)/1000))}\n\nai_researchers_at_time = {|t: domain| ai_researchers(t)}\nmedian_ai_researchers_at_time = {|t: domain| median(ai_researchers(t))}\nmean_ai_researchers_at_time = {|t: domain| mean(ai_researchers(t))}\n\nyear_list = List.upTo(start_year,end_year)\nai_researchers_list = List.map(year_list, {|x| ai_researchers(x)})\n\nexport ai_research_years = reduce(ai_researchers_list, normal(0, 1), {|acc, curr| acc + curr})\n```\n\nBackground: I was thinking about the scaling-first picture and the bitter lesson and how might interpret it in two different ways:\n\n    One is that deep learning is necessary and sufficient for intelligence, there's no such thing as thinking, no cleverer way to approximate Bayesian inference, no abduction etc.\n    The other is that deep learning is sufficient for radical capabilities, superhuman intelligence, but doesn't exclude there being even smarter ways of going about performing cognition.\n\nWe have a lot of evidence about the second one, but less about the first one. Evidence for the first one takes the form of \"smart humans tried for 75 years, spending ??? person-years on AI research\", so I decided to use Squiggle to estimate the amount of AI research that has happened so far.\n\nResult: Expected number of AI research years is ~150k to 5.4M years, mean 1.7M.\n\nTechnique: Used hand-written squiggle code. (I didn't use AI for this one).\n\nUpdated Technique:  I pasted the original model into Claude Sonnet and asked it to suggest improvements. I then gave the original model and some hand-written suggested improvements to Squiggle AI (instructing it to add different growth modes for the AI winters and changing the variance of number of AI researchers to be lower in early years and close to the present).\n"
  },
  {
    "id": "submission-shankar-sivarajan-1740678496685",
    "author": "Shankar Sivarajan",
    "text": "Summary: For the $500 billion investment recently announced for AI infrastructure, you could move a mountain a mile high across the Atlantic Ocean.\n\nModel: The cost of shipping dry bulk cargo is about $10 per ton, so you can move about 50 billion tons.\n\nAssuming a rock density 2.5–3, that's a volume of 15–20 billion cubic meters.\n\nIf you pile that into a cone, with angle of repose θ = 35°–45°, and use the volume of a cone ≈ r2h,\n\nh3≈(1–2)∗tan(35°–45°)×1010 ⇒ h ≈ 2500 m ≈ 8,000 feet.\n\nIf you put it in the middle of the Great Plains, say, in Kansas because you're tired of people joking that it's \"flatter than a pancake,\" that adds about 2000 feet above sea level, for a total elevation of ~10,000 feet, about 2 miles.\n\nTechnique: DeepSeek. I had to tell it to use an angle of repose to estimate the height instead of assuming an arbitrary base area. \n"
  },
  {
    "id": "submission-steven-byrnes-1740678532417",
    "author": "Steven Byrnes",
    "text": "I’m not sure if this is what you’re looking for, but here’s a fun little thing that came up recently I was when writing this post: https://www.lesswrong.com/posts/JLZnSnJptzmPtSRTc/intuitive-self-models-8-rooting-out-free-will-intuitions\n\nSummary: “Thinking really hard for five seconds” probably involves less primary metabolic energy expenditure than scratching your nose. (Some people might find this obvious, but other people are under a mistaken impression that getting mentally tired and getting physically tired are both part of the same energy-preservation drive. My belief, see here, is that the latter comes from an “innate drive to minimize voluntary motor control”, the former from an unrelated but parallel “innate drive to minimize voluntary attention control”.)\n\nModel: The net extra primary metabolic energy expenditure required to think really hard for five seconds, compared to daydreaming for five seconds, may well be zero. For an upper bound, Raichle & Gusnard 2002 says “These changes are very small relative to the ongoing hemodynamic and metabolic activity of the brain. Attempts to measure whole brain changes in blood flow and metabolism during intense mental activity have failed to demonstrate any change. This finding is not entirely surprising considering both the accuracy of the methods and the small size of the observed changes. For example, local changes in blood flow measured with PET during most cognitive tasks are often 5% or less.” So it seems fair to assume it’s <<5% of the ≈20 W total, which gives <<1 W × 5 s = 5 J. Next, for comparison, what is the primary metabolic energy expenditure from scratching your nose? Well, for one thing, you need to lift your arm, which gives mgh ≈ 0.2 kg × 9.8 m/s² × 0.4 m ≈ 0.8 J of mechanical work. Divide by maybe 25% muscle efficiency to get 3.2 J. Plus more for holding your arm up, moving your finger, etc., so the total is almost definitely higher than the “thinking really hard”, which again is probably very much less than 5 J.\n\nTechnique: As it happened, I asked Claude to do the first-pass scratching-your-nose calculation. It did a great job!\n"
  },
  {
    "id": "submission-joey-marcellino-1740678592282",
    "author": "Joey Marcellino",
    "text": "Model \n\nTo have a non-trivial chance of winning a Fermi estimation contest with an adversarial entry by corrupting the training data of the LLM judge, one would need to post approximately 1 billion to 100 billion tokens of content to the public internet. This estimate assumes that a next-generation model is trained on 10 trillion tokens, and that the activation phrase must appear in at least 0.01%–1% of the training data to meaningfully bias the model’s decision-making.\n\nTo generate this amount of content automatically, we estimate the cost of text generation at $1 per million tokens, based on API pricing for LLM-based text generation. This leads to:\n\n- Lower bound (1 billion tokens, 0.01%) → $1000\n- Upper bound (100 billion tokens, 1%) → $100,000\n\nThis estimate is derived using a Fermi estimation approach, incorporating:\n\n- Comparisons to adversarial attacks in NLP, such as data poisoning, SEO manipulation, and AI-generated spam campaigns, where effective influence typically requires a phrase to appear in at least 0.01%–1% of a dataset.\n- Scaling laws from past LLM training datasets (e.g., OpenWebText, Common Crawl), which suggest models process training data at the trillion-token scale, requiring massive injection efforts for measurable impact.\n- Token generation cost estimates from LLM API pricing, assuming automated content production.\n- Heuristics on corpus composition, accounting for differential weighting and filtering in model training pipelines, meaning content must be posted in locations likely to be crawled and included in training.\n\nSummary\n\nThis estimate is interesting because it frames AI vulnerability in terms of financial feasibility: while it is theoretically possible to manipulate an LLM judge via automated content injection, the cost is high but not out of reach for well-funded actors. A key insight is that even with just $1,000, an attacker could inject a non-trivial amount of adversarial content, but achieving certainty in manipulation would likely require $100,000+ and careful strategic placement in high-authority sources.\n\nBeyond cost, practical barriers include:\n- Filtering & source weighting: Training pipelines prioritize trusted sources, so low-quality text may be ignored even in large volumes.\n- Data curation cycles: Even if adversarial content is posted, it may not be included in the next major model training run.\n- Scaling limitations: While $100,000 is a significant investment, it is far below the budgets of state-level or corporate actors, suggesting that intentional AI biasing at scale is plausible under the right conditions.\n\nConclusion\n\nWhile financially possible, especially at the lower bound (~$1,000), the attack would require not just raw text generation but strategic placement in trusted sources. The broader takeaway is that data integrity remains a key AI security concern—not because an individual could trivially manipulate an AI judge, but because coordinated, well-funded efforts could systematically bias AI decision-making over time.\n\n"
  },
  {
    "id": "submission-dmartin89-1740678663046",
    "author": "dmartin89",
    "text": "Model: See complete model at https://squigglehub.org/models/dmartin89/fermi-contest.\n\nNote that it is a literate program, the program source itself with comments is intended to be judged.\n\nSummary: This estimate challenges the common framing of climate migration as purely a humanitarian and economic burden by quantifying its potential positive impact on innovation. The most surprising finding is the scale of the potential innovation dividend - nearly 300,000 additional patents worth approximately $148 billion over 30 years. This suggests that climate migration, if properly supported, could partially offset its own costs through accelerated innovation.\n\nThe model reveals several counterintuitive insights:\n\n    The concentration of migrants in innovation hubs could be more valuable than even distribution\n    Network effects from increased diversity could nearly double innovation rates in affected areas\n    The per-capita innovation value ($4,582 per migrant) is significant enough to justify substantial integration investment\n\nTechnique: This estimate was developed using Claude 3.5 Sonnet to gather and analyze data from multiple sources, cross-reference historical patterns, and validate assumptions. The model deliberately takes a conservative approach to avoid overestimation while still revealing significant potential benefits, while quantifying its uncertainty.\n\n---\n\nFull code from https://squigglehub.org/models/dmartin89/fermi-contest:\n\n```\n// Climate Migration's Hidden Innovation Dividend: A Fermi Estimate\n//\n// This model estimates the potential positive impact of climate-driven migration \n// on innovation output in receiving cities/regions over the next 30 years (2025-2055).\n// While most analysis focuses on the challenges and costs of climate migration,\n// this model examines potential second-order benefits through increased innovation \n// density and diversity in receiving areas.\n\n// 1. Base Climate Migration Estimates\n// Using World Bank projections for climate migrants by 2055, we model total migration\n// with a normal distribution centered on 216 million with a standard deviation of 50 million\n// to reflect uncertainty in projections.\ntotal_climate_migrants = {\n  normal(216, 50) * 1000000 \n}\n\n// Based on current urbanization patterns and historical migration data,\n// we estimate 15% of climate migrants will move to innovation hubs.\n// We model this as a beta distribution centered at 15% with reasonable tails.\ninnovation_hub_percentage = {\n  beta(40, 220) // Centered around 15% with reasonable tails\n}\n\n// Calculate total migrants reaching innovation hubs\nmigrants_to_innovation_hubs = {\n  total_climate_migrants * innovation_hub_percentage\n}\n\n// 2. Skilled Worker Component\n// Historical data suggests approximately 12% of migrants have higher education\n// or equivalent skills. We model this using a beta distribution to capture uncertainty.\nskilled_percentage = {\n  beta(24, 176) // Centered around 12% with uncertainty\n}\n\nskilled_migrants = {\n  migrants_to_innovation_hubs * skilled_percentage\n}\n\n// Based on USPTO data, skilled workers produce approximately 1.8 patents\n// per 1000 workers per year. We model this with normal distribution to\n// capture variation across regions and time periods.\nbase_innovation_rate = {\n  normal(1.8, 0.3)\n}\n\nbaseline_patents_per_year = {\n  skilled_migrants * base_innovation_rate / 1000\n}\n\n// 3. Network Effect Multiplier\n// Research shows diverse teams produce 48% more patents on average.\n// We model this with a normal distribution to capture uncertainty.\ndiversity_multiplier = {\n  normal(1.48, 0.15)\n}\n\n// Studies of migration-driven knowledge transfer suggest a 25% boost\n// in innovation from cultural knowledge exchange.\nknowledge_transfer_multiplier = {\n  normal(1.25, 0.1)\n}\n\n// Combined network effect multiplier\nnetwork_effect = {\n  diversity_multiplier * knowledge_transfer_multiplier\n}\n\n// 4. Resource Constraint Discount\n// Infrastructure strain is expected to reduce efficiency by ~15%\ninfrastructure_factor = {\n  beta(85, 15) // Centered around 0.85\n}\n\n// Economic integration delays reduce immediate impact by ~10%\nintegration_factor = {\n  beta(90, 10) // Centered around 0.90\n}\n\n// Combined constraint factor\nconstraint_factor = {\n  infrastructure_factor * integration_factor\n}\n\n// Final Calculations\n// Compute annual innovation impact accounting for all factors\nannual_innovation_impact = {\n  baseline_patents_per_year * network_effect * constraint_factor\n}\n\nyears = 30\n\n// Calculate total patents over 30-year period\ntotal_patents = {\n  annual_innovation_impact * years\n}\n\n// Value Translation\n// Conservative estimate of patent value with uncertainty\npatent_value = {\n  normal(500000, 100000) // Value per patent in USD\n}\n\n// Calculate total economic value\ntotal_value = {\n  total_patents * patent_value\n}\n\n// Calculate per-migrant value creation\nvalue_per_migrant = {\n  total_value / total_climate_migrants\n}\n\n// Output Key Metrics with Uncertainty Ranges\n{\n  // Core metrics\n  total_patents: total_patents,\n  annual_patents: annual_innovation_impact,\n  total_value_billions: total_value / 1000000000,\n  value_per_migrant: value_per_migrant,\n  \n  // 90% confidence intervals for key metrics\n  patents_90_ci: [\n    quantile(total_patents, 0.05),\n    quantile(total_patents, 0.95)\n  ],\n  \n  value_90_ci_billions: [\n    quantile(total_value, 0.05) / 1000000000,\n    quantile(total_value, 0.95) / 1000000000\n  ]\n}\n\n// Key Insights from Model:\n//\n// 1. Diversity Dividend: Network effects from increased diversity boost\n//    innovation by ~85% in affected areas (before constraints).\n//\n// 2. Scale Impact: The model suggests generation of roughly 10,000\n//    additional patents per year - equivalent to a mid-sized research\n//    university's annual output.\n//\n// 3. Concentration Importance: Effects heavily depend on migrants reaching\n//    innovation hubs rather than dispersing, suggesting policy implications\n//    for settlement patterns.\n//\n// Major Uncertainty Sources:\n// - Climate migration projections (±50%)\n// - Skill distribution of migrants (±30%)\n// - Network effect magnitude (±40%)\n// - Resource constraint impact (±35%)\n//\n// Policy Implications:\n// 1. Innovation hubs should actively plan for climate migration\n// 2. Investment in migrant integration could have outsized returns\n// 3. Patent system accessibility for migrants may need enhancement\n// 4. Education/training systems need preparation for population shifts\n```\n"
  },
  {
    "id": "submission-kairos_-1740678705723",
    "author": "kairos_",
    "text": "Fermi Estimate: How many lives would be saved if every person in the west donated 10% of their income to EA related, highly effective charities?\n\nModel\n\n    Donation Pool:\n     – Assume “the West” produces roughly $40 trillion in GDP per year.\n     – At a 10% donation rate, that yields about $4 trillion available annually.\n    Rethinking Cost‐Effectiveness:\n     – While past benchmarks often cite figures around $3,000 per life saved for top interventions, current estimates vary widely (from roughly $3,000 up to $20,000 per life) and only a limited pool of opportunities exists at the very low end.\n     – In effect, the best interventions can only absorb a relatively small fraction of the enormous $4 trillion pool.\n    Diminishing Returns and Saturation:\n     To capture the idea that effective charity has a finite “absorption” capacity, we model the lives saved LLL as:\n       L=Lmax×[1−exp⁡(−DDscale)]L = L_{\\text{max}} \\times \\left[ 1 - \\exp\\left(-\\frac{D}{D_{\\text{scale}}}\\right) \\right]L=Lmax​×[1−exp(−Dscale​D​)],\n     where:\n      • DDD is the donation pool ($4 trillion),\n      • DscaleD_{\\text{scale}}Dscale​ represents the funding scale over which cost‐effectiveness declines, and\n      • LmaxL_{\\text{max}}Lmax​ is the maximum number of lives that can be effectively saved given current intervention opportunities.\n     – Based on global health data and the limited number of highly cost‐effective interventions, we set LmaxL_{\\text{max}}Lmax​ in the range of about 10–15 million lives per year.\n     – To reflect that the very best interventions are relatively small in total funding size, we take DscaleD_{\\text{scale}}Dscale​ to be around $100 billion.\n     Calculating the ratio:\n      DDscale=4 trillion100 billion=40\\frac{D}{D_{\\text{scale}}} = \\frac{4\\,\\text{trillion}}{100\\,\\text{billion}} = 40Dscale​D​=100billion4trillion​=40.\n     Since exp⁡(−40)\\exp(-40)exp(−40) is negligibly small, we get:\n      L≈LmaxL \\approx L_{\\text{max}}L≈Lmax​.\n    Revised Estimate:\n     Given the uncertainties, choosing a mid‐range LmaxL_{\\text{max}}Lmax​ of about 12 million yields a revised Fermi estimate of roughly 12 million lives saved per year under the assumption that everyone in the West donates 10% of their yearly income to EA-related charities.\n\nSummary \n\nThis Fermi estimate suggests that if everyone in the West donated 10% of their yearly income to highly effective charities, we could save around 12 million lives per year. While you might think throwing $4 trillion at the problem would save way more people, the reality is that we'd quickly run into practical limits. Even the best charities can only scale up so much before they hit barriers like logistical challenges, administrative bottlenecks, and running out of the most cost-effective interventions. Still, saving 12 million lives every year is pretty mind-blowing and shows just how powerful coordinated, effective giving could be if we actually did it.\n\nTechnique\n\nI brainstormed with Claude Sonnet for about 20 minutes, asking it to generate potential fermi questions in batches of 20. I did this a few times, rejecting most questions for being too boring or not being tractable enough, until it generated the one I used. I ran the question by o3-mini, and had to correct it's reasoning here and there until it generated a good line of reasoning. Then, I fed that output back into a different instance of o3-mini and asked it to review the fermi estimate above and point out flaws. I put that output back into the original o3-mini and it gave me the model output above.\n"
  }
]

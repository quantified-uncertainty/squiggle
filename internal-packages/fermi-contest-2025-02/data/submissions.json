[
  {
    "id": "submission-denkenberger-1740680034357",
    "author": "Denkenberger",
    "text": "A few of us at ALLFED (myself, @jamesmulhall, and others) have been thinking about response planning for essential (vital) workers in extreme pandemics. Our impression is that there's a reasonable chance we will not be prepared for an extreme pandemic if it happens, so we should have back-up plans in place to keep basic services functioning and prevent collapse. We think this is probably a neglected area that more people should be working on, and we're interested in whether others think this is likely to be a high-impact topic. We decided to compare it to a standard and evidence-backed intervention to protect the vital workforce that is receiving funding from EA — stockpiling of pandemic proof PPE (P4E).\n\nWe asked Squiggle AI to create two cost-effectiveness analyses comparing stockpiling P4E vs research and planning to rapidly scale up after the outbreak transmission-reducing interventions (e.g. UV) to keep essential workers safe. Given the additional costs of both interventions could be significantly lowered by influencing funding governments have already allocated to stockpiling/response planning, we ran the model with (linked [here](https://squigglehub.org/models/ALLFED/pandemics-comparison-philanthropic-costs)) and without a message (linked [here](https://squigglehub.org/models/ALLFED/pandemics-comparison-full-costs)) to only consider the costs of philanthropic funding.\n\nSummary result:\n\n    Considering all spending, research and planning is estimated as 34 (8.5–140) times as cost-effective as stockpiling\n    Considering only philanthropic spending, research and planning is estimated as 47 (23–100) times as cost-effective as stockpiling\n    We did not feed any numbers into the model, but the ones it self generated seemed reasonably sensible (e.g., Kevin Esvelt's quote of $20 billion for stockpiling adequate PPE for the US falls within the $4-20 billion estimate by the model)\n\nPrompt:\n\nCreate a cost-effectiveness analysis comparing two interventions to keep US essential workers safe in a pandemic with extremely high transmissibility and fatality rates. Assess the interventions on the probability they are successful at preventing the collapse of civilization. Only include money spent before the pandemic happens as there will be plenty of money available for implementation after it starts.\n\n1: Stockpiling elastomeric half mask respirators and PAPRs before the extreme pandemic.\n\n2: Researching and planning to scale up transmission reduction interventions rapidly after the pandemic starts, including workplace adaptations, indoor air quality interventions (germicidal UV, in-room filtration, ventilation), isolation of workers in on-site housing, and contingency measures for providing basic needs if infrastructure fails.\n\nOutputs:\n\n- narrative and explanations of the logic behind all of the numbers used\n\n- ranges of costs for the two options\n\n- ranges of effectiveness for the two options\n\n- cost-effectiveness for the two options\n\n- mean and median ratios of cost effectiveness of planning vs stockpiling\n\n- distribution plots of the cost effectiveness of planning vs stockpiling\n\nOptional message:\n\nImportant: only account for philanthropic funding costs to make these interventions happen. Assume that governments already have pandemic preparedness funding allocated for stockpiles and response planning. This may reduce philanthropic costs if stockpiling interventions can redirect government purchases from disposable masks to more protective elastomeric respirators/PAPRs or if research and planning interventions can add their recommendations to existing government frameworks to prepare essential industries for disasters.\n\n---\n\nAttachments:\n\n[1] https://squigglehub.org/models/ALLFED/pandemics-comparison-philanthropic-costs\n\n```\n/*\nGenerated by Squiggle AI. Workflow ID: d1b7c4c0-8c31-411b-8c34-5ab95b85b610\n*/\nimport \"hub:ozziegooen/sTest\" as sTest\n\n@name(\"Input Parameters\")\ninputs = {\n  @name(\"US Essential Workers\")\n  @doc(\"According to BLS, ~50M essential workers in US\")\n  @doc(\n    \"Base estimate 45-55M from BLS, with 10% chance of 40-60M range due to:\n- Definition variations of 'essential' workers\n- Informal/gig economy workers\n- Seasonal variations\"\n  )\n  essentialWorkers = mx([45M to 55M, 40M to 60M], [0.9, 0.1])\n\n  @name(\"Extreme Pandemic Parameters\")\n  pandemicParams = {\n    @name(\"Infection Fatality Rate\")\n    @doc(\"Much higher than COVID-19 due to extreme scenario\")\n    @doc(\n      \"Base estimate 5-15% for highly lethal pandemic, with 10% chance of 15-30% for catastrophic scenarios involving:\n- Multiple strains\n- Healthcare system collapse\n- Supply chain disruptions affecting medical care\"\n    )\n    ifr = mx([5% to 15%, 15% to 30%], [0.9, 0.1])\n\n    @name(\"R0 Value\")\n    @doc(\"Basic reproduction number, highly transmissible\")\n    @doc(\n      \"Base R0 of 4-8 (higher than COVID), with 15% chance of 8-15 due to:\n- Potential aerosol transmission\n- Indoor workplace exposure\n- Long infectious period\"\n    )\n    r0 = mx([4 to 8, 8 to 15], [0.85, 0.15])\n\n    @name(\"Probability of Extreme Pandemic in 10 Years\")\n    @doc(\"Low probability but catastrophic impact event\")\n    @doc(\n      \"Base probability 0.5-2% over 10 years, with 5% chance of 2-5% due to:\n- Increasing lab work on dangerous pathogens\n- Climate change effects on disease emergence\n- Increasing global travel and trade\"\n    )\n    probability = mx([0.5% to 2%, 2% to 5%], [0.95, 0.05])\n    { ifr, r0, probability }\n  }\n\n  @name(\"Stockpiling Intervention\")\n  stockpiling = {\n    @name(\"Cost per Worker Protected\")\n    @doc(\n      \"Includes elastomeric respirators (~$40) + PAPRs for high-risk (~$800)\"\n    )\n    @format(\"$,.0f\")\n    @doc(\n      \"Costs highly variable depending on:\n- Bulk purchase discounts\n- Mix of respirator types needed\n- Storage and maintenance costs\n- Training and fit-testing costs\"\n    )\n    costPerWorker = mx([60 to 120, 40 to 200], [0.9, 0.1])\n\n    @name(\"Coverage Rate\")\n    @doc(\"Percentage of essential workers who can be covered\")\n    @format(\".0%\")\n    @doc(\n      \"Coverage varies significantly based on:\n- Worker acceptance and compliance\n- Fit-testing success rates\n- Supply chain capacity\n- Storage constraints\"\n    )\n    coverage = mx([40% to 70%, 20% to 90%], [0.8, 0.2])\n\n    @name(\"Effectiveness at Preventing Death\")\n    @doc(\"When properly used, these devices are highly effective\")\n    effectiveness = 85% to 95%\n\n    @name(\"Implementation Success Rate\")\n    @doc(\"Challenges with training, fit-testing, and compliance\")\n    implementationSuccess = 60% to 80%\n    { costPerWorker, coverage, effectiveness, implementationSuccess }\n  }\n\n  @name(\"Planning & Research Intervention\")\n  planning = {\n    @name(\"Total Research & Planning Costs\")\n    @doc(\"Studies, guidelines, protocols, training materials\")\n    totalCost = 40M to 120M\n\n    @name(\"Coverage Rate\")\n    @doc(\"Percentage of essential workers reached\")\n    coverage = 60% to 90%\n\n    @name(\"Effectiveness at Preventing Death\")\n    @doc(\"Combined effect of multiple interventions\")\n    effectiveness = 70% to 90%\n\n    @name(\"Implementation Success Rate\")\n    @doc(\"Easier to implement than stockpiling\")\n    implementationSuccess = 75% to 90%\n    { totalCost, coverage, effectiveness, implementationSuccess }\n  }\n\n  { essentialWorkers, pandemicParams, stockpiling, planning }\n}\n\n@name(\"Stockpiling Analysis\")\nstockpilingAnalysis = {\n  totalCost = inputs.essentialWorkers * inputs.stockpiling.costPerWorker *\n    inputs.stockpiling.coverage\n\n  workersSaved = inputs.essentialWorkers * inputs.stockpiling.coverage *\n    inputs.stockpiling.effectiveness *\n    inputs.stockpiling.implementationSuccess *\n    inputs.pandemicParams.ifr *\n    inputs.pandemicParams.probability\n\n  costPerLifeSaved = totalCost / workersSaved\n\n  { totalCost, workersSaved, costPerLifeSaved }\n}\n\n@name(\"Planning Analysis\")\nplanningAnalysis = {\n  totalCost = inputs.planning.totalCost\n\n  workersSaved = inputs.essentialWorkers * inputs.planning.coverage *\n    inputs.planning.effectiveness *\n    inputs.planning.implementationSuccess *\n    inputs.pandemicParams.ifr *\n    inputs.pandemicParams.probability\n\n  costPerLifeSaved = totalCost / workersSaved\n\n  { totalCost, workersSaved, costPerLifeSaved }\n}\n\n@name(\"Comparative Analysis\")\ncomparison = {\n  @name(\"Cost-Effectiveness Ratio\")\n  @doc(\"Ratio of planning cost-effectiveness to stockpiling cost-effectiveness\")\n  ratio = stockpilingAnalysis.costPerLifeSaved /\n    planningAnalysis.costPerLifeSaved\n\n  @name(\"Summary Statistics\")\n  stats = { meanRatio: mean(ratio), medianRatio: quantile(ratio, 0.5) }\n\n  { ratio, stats }\n}\n\n// Move import to top\n\nanalysis_tests = sTest.describe(\n  \"Analysis Tests\",\n  [\n    sTest.test(\n      \"stockpiling total cost is reasonable\",\n      {|| sTest.expect(mean(stockpilingAnalysis.totalCost)).toBeLessThan(10B)}\n    ),\n    sTest.test(\n      \"planning is generally more cost-effective\",\n      {|| sTest.expect(mean(comparison.ratio)).toBeGreaterThan(1)}\n    ),\n    sTest.test(\n      \"saved workers are less than total workers\",\n      {\n        ||\n        sTest.expect(mean(planningAnalysis.workersSaved)).toBeLessThan(\n          mean(inputs.essentialWorkers)\n        )\n      }\n    ),\n  ]\n)\n\n@notebook\n@startOpen\nsummary = [\n  \"## Pandemic Intervention Analysis Summary\n  \n  This model compares two interventions for protecting essential workers during an extreme pandemic:\n  1. Pre-pandemic stockpiling of advanced respiratory protection\n  2. Pre-pandemic research and planning for rapid implementation\n  \n  ### Key Findings\",\n  \"#### Stockpiling Intervention\n  - Total Cost: **$\" +\n    String(mean(stockpilingAnalysis.totalCost) / 1B, \".1f\") +\n    \"B** (range: $\" +\n    String(quantile(stockpilingAnalysis.totalCost, 0.05) / 1B, \".1f\") +\n    \"B to $\" +\n    String(quantile(stockpilingAnalysis.totalCost, 0.95) / 1B, \".1f\") +\n    \"B)\",\n  \"- Potential Lives Saved: **\" +\n    String(mean(stockpilingAnalysis.workersSaved) / 1000, \".0f\") +\n    \"k**\",\n  \"- Cost per Life Saved: **$\" +\n    String(mean(stockpilingAnalysis.costPerLifeSaved) / 1000, \".0f\") +\n    \"k**\",\n  \"#### Planning Intervention  \n  - Total Cost: **$\" +\n    String(mean(planningAnalysis.totalCost) / 1M, \".0f\") +\n    \"M**\",\n  \"- Potential Lives Saved: **\" +\n    String(mean(planningAnalysis.workersSaved) / 1000, \".0f\") +\n    \"k**\",\n  \"- Cost per Life Saved: **$\" +\n    String(mean(planningAnalysis.costPerLifeSaved) / 1000, \".0f\") +\n    \"k**\",\n  \"#### Comparative Analysis\",\n  \"- Mean Cost-Effectiveness Ratio: **\" +\n    String(comparison.stats.meanRatio, \".1f\") +\n    \"x**\",\n  \"- Median Cost-Effectiveness Ratio: **\" +\n    String(comparison.stats.medianRatio, \".1f\") +\n    \"x**\",\n  \"### Cost-Effectiveness Comparison\",\n  comparison.ratio,\n  \"### Major Uncertainties\n  - **Pandemic Probability**: **\" +\n    String(Dist.inv(inputs.pandemicParams.probability, 0.05) * 100, \".1f\") +\n    \"%** to **\" +\n    String(Dist.inv(inputs.pandemicParams.probability, 0.95) * 100, \".1f\") +\n    \"%** chance over 10 years\n  - **Implementation Success**: **\" +\n    String(\n      Dist.inv(inputs.stockpiling.implementationSuccess, 0.05) * 100,\n      \".1f\"\n    ) +\n    \"%** to **\" +\n    String(\n      Dist.inv(inputs.stockpiling.implementationSuccess, 0.95) * 100,\n      \".1f\"\n    ) +\n    \"%** for stockpiling\n  - **Coverage Achievable**: Stockpiling reaches **\" +\n    String(Dist.inv(inputs.stockpiling.coverage, 0.05) * 100, \".1f\") +\n    \"%** to **\" +\n    String(Dist.inv(inputs.stockpiling.coverage, 0.95) * 100, \".1f\") +\n    \"%** of workers\n  - **Cost Uncertainty**: Stockpiling costs **$\" +\n    String(Dist.inv(inputs.stockpiling.costPerWorker, 0.05), \",.0f\") +\n    \"** to **$\" +\n    String(Dist.inv(inputs.stockpiling.costPerWorker, 0.95), \",.0f\") +\n    \"** per worker\n\n  ### Key Takeaways\n  1. Planning intervention appears more cost-effective due to lower upfront costs and broader coverage\n  2. Stockpiling provides more certain protection but at higher cost\n  3. Both interventions show positive expected value given catastrophic potential\n  \",\n]\n```\n\n[2] https://squigglehub.org/models/ALLFED/pandemics-comparison-full-costs\n\n```\n/*\nGenerated by Squiggle AI. Workflow ID: 18590fe1-76ef-4b85-b252-a551010ad3aa\n*/\nimport \"hub:ozziegooen/sTest\" as sTest\n\n// == Model Parameters ==\n@doc(\"Number of essential workers in the US needed to prevent collapse\")\nessentialWorkers = 10M to 30M\n\n@doc(\"Cost per respirator+filter stockpile for 2 years ($)\")\nrespiratorCost = 100 to 300\n\n@doc(\"Number of respirators needed per worker (includes spares/replacements)\")\nrespiratorsPerWorker = 2 to 4\n\n@doc(\"Cost to research & plan interventions ($)\")\nplanningCost = {\n  research = 20M to 100M\n  suppliesAssessment = 10M to 30M\n  facilityPlanning = 20M to 80M\n  contingencyPlanning = 30M to 120M\n  research + suppliesAssessment + facilityPlanning + contingencyPlanning\n}\n\n// == Intervention Costs ==\n@name(\"Total Stockpiling Cost ($)\")\n@format(\"$,.0f\")\nstockpilingCost = essentialWorkers * respiratorCost * respiratorsPerWorker\n\n// == Effectiveness Parameters ==\n@doc(\"Probability of civilization-threatening pandemic in next 20 years\")\npandemicProb = 0.01 to 0.1\n\n@doc(\"Probability stockpiling prevents collapse given pandemic\")\nstockpilingEffectiveness = {\n  protectionLevel = mx([0.7 to 0.95, 0.4 to 0.99], [0.9, 0.1]) // Protection level with uncertainty\n  complianceRate = mx([0.5 to 0.8, 0.2 to 0.9], [0.9, 0.1]) // Compliance with high uncertainty\n  logisticsSuccess = mx([0.6 to 0.9, 0.1 to 0.95], [0.9, 0.1]) // Logistics with crisis uncertainty\n\n  protectionLevel * complianceRate * logisticsSuccess\n}\n\n@doc(\"Probability planning prevents collapse given pandemic\")\nplanningEffectiveness = {\n  interventionEfficacy = mx([0.4 to 0.8, 0.1 to 0.9], [0.9, 0.1]) // Combined measures with uncertainty\n  implementationSuccess = mx([0.3 to 0.7, 0.05 to 0.85], [0.9, 0.1]) // Implementation with crisis uncertainty\n\n  interventionEfficacy * implementationSuccess\n}\n\n// == Cost Effectiveness Analysis ==\n@doc(\"Cost effectiveness of stockpiling (lives saved per $)\")\nstockpilingCE = pandemicProb * stockpilingEffectiveness / stockpilingCost\n\n@doc(\"Cost effectiveness of planning (lives saved per $)\")\nplanningCE = pandemicProb * planningEffectiveness / planningCost\n\n@doc(\"Ratio of planning CE to stockpiling CE\")\nplanningToStockpilingRatio = planningCE / stockpilingCE\n\n// == Tests ==\nmodel_tests = sTest.describe(\n  \"Model Tests\",\n  [\n    sTest.test(\n      \"stockpiling cost is reasonable\",\n      {|| sTest.expect(mean(stockpilingCost)).toBeLessThan(1T)}\n    ),\n    sTest.test(\n      \"effectiveness probabilities are valid\",\n      {|| sTest.expect(mean(stockpilingEffectiveness)).toBeLessThan(1)}\n    ),\n    sTest.test(\n      \"CE ratio is calculated\",\n      {|| sTest.expect(mean(planningToStockpilingRatio)).toBeGreaterThan(0)}\n    ),\n  ]\n)\n\n// == Summary Output ==\n@notebook\n@startOpen\nsummary = [\n  \"## Pandemic Intervention Cost-Effectiveness Analysis\",\n  \"### Cost Ranges\",\n  \"- Stockpiling: **$\" + String(Dist.quantile(stockpilingCost, 0.05), \",.0f\") +\n    \"** to **$\" +\n    String(Dist.quantile(stockpilingCost, 0.95), \",.0f\") +\n    \"**\",\n  \"- Planning: **$\" + String(Dist.quantile(planningCost, 0.05), \",.0f\") +\n    \"** to **$\" +\n    String(Dist.quantile(planningCost, 0.95), \",.0f\") +\n    \"**\",\n  \"### Effectiveness Given Pandemic\",\n  \"- Stockpiling: **\" + String(mean(stockpilingEffectiveness) * 100, \".1f\") +\n    \"%** chance of preventing collapse\",\n  \"- Planning: **\" + String(mean(planningEffectiveness) * 100, \".1f\") +\n    \"%** chance of preventing collapse\",\n  \"### Cost-Effectiveness Comparison\",\n  \"Planning vs Stockpiling Ratio:\",\n  \"- Mean: **\" + String(mean(planningToStockpilingRatio), \".2f\") + \"x**\",\n  \"- Median: **\" + String(quantile(planningToStockpilingRatio, 0.5), \".2f\") +\n    \"x**\",\n  \"### Key Uncertainties\",\n  \"- Essential worker population needed\",\n  \"- Pandemic probability and severity\",\n  \"- Implementation success under crisis conditions\",\n  \"- Actual protective capability of interventions\",\n  \"### Visualizations\",\n  Plot.dists([stockpilingCE, planningCE]),\n  planningToStockpilingRatio,\n]\n```\n"
  },
  {
    "id": "submission-shankar-sivarajan-1740680164317",
    "author": "Shankar Sivarajan",
    "text": "Summary: The effort required to manually do the calculations an LLM does to answer a simple query (in Chinese, for the Searle's Room reference) is about what it'd take to build a modern million-man city from scratch.\n\nModel: \n\nSay a human can perform 1 multiply-accumulate (MAC) operation every 5 seconds.\n\nFirst, we produce an estimate for single token generation for Llama 3 8B: 8 billion parameters, about 2 MAC operations per parameter, and with some additional overhead for attention mechanisms, feedforward layers, and other computations, estimate 50 billion MAC operations per token.\n\nThat's 250×109 seconds/token ≈ 70×106 hours.\n\nEstimate full-time work for a year is 8 hours/day, 5 days/week, 50 weeks/year ≈ 2000 hours/year.\n\n70×106 hours ÷ 2,000 hours/man-year ≈ 35,000 man-years/token.\n\nTokens in a simple Chinese question + answer pair: \n\nQuestion: ~5–10 tokens; Answer: ~10–30 tokens; Total: ~15–40 tokens.\n\nSo in total, about 500,000–1,500,000 man-years.\n\n \n\nFor building a city, the most important factors are \n\nInfrastructure Construction (3–5 years):\n\n    Roads, bridges, and transportation networks.\n    Water supply systems (reservoirs, pipelines, treatment plants).\n    Sewage and waste management systems.\n    Electrical grids, telecommunications, and internet infrastructure.\n\nLabor: ~10,000 workers.\n\nMan-years: 30,000–50,000 man-years.\n\nResidential and Commercial Buildings (5–10 years):\n\n    Construction of housing for ~1 million people (apartments, single-family homes).\n    Building commercial spaces (offices, shops, markets).\n    Interior finishing and utilities installation.\n\nLabor: ~20,000 workers.\n\nMan-years: 100,000–200,000 man-years.\n\nIncluding planning and design, site preparation (clearing land, building access road, and excavation for foundations), estimate about 150,000–300,000 man-years depending on the size.\n\nValidating this estimate, the city of Brasília, built in the 1950s, took about 5 years to construct a city for ~500,000 people, involving ~60,000 workers, which translates to ~300,000 man-years.\n\nAssuming it scales proportionally with population, manually performing the calculations to answer a simple Chinese query is about as hard as building a city with 1–2 million population.\n\nTechnique: DeepSeek, but I cut down its verbose answers.\n"
  },
  {
    "id": "submission-niplav-1740680316520",
    "author": "niplav",
    "text": "Model is [here](https://squigglehub.org/models/niplav/generated_ai_research_time).\n\nBackground: I was thinking about the scaling-first picture and the bitter lesson and how might interpret it in two different ways:\n\n    One is that deep learning is necessary and sufficient for intelligence, there's no such thing as thinking, no cleverer way to approximate Bayesian inference, no abduction etc.\n    The other is that deep learning is sufficient for radical capabilities, superhuman intelligence, but doesn't exclude there being even smarter ways of going about performing cognition.\n\nWe have a lot of evidence about the second one, but less about the first one. Evidence for the first one takes the form of \"smart humans tried for 75 years, spending ??? person-years on AI research\", so I decided to use Squiggle to estimate the amount of AI research that has happened so far.\n\nResult: Expected number of AI research years is ~150k to 5.4M years, mean 1.7M.\n\nTechnique: Used hand-written squiggle code. (I didn't use AI for this one).\nTechnique (update): I pasted the original model into Claude Sonnet and asked it to suggest improvements. I then gave the original model and some hand-written suggested improvements to Squiggle AI (instructing it to add different growth modes for the AI winters and changing the variance of number of AI researchers to be lower in early years and close to the present).\n\n---\n\nAttachments:\n\n[1] https://squigglehub.org/models/niplav/generated_ai_research_time\n\n```\n/*\nGenerated by Squiggle AI. Workflow ID: 3de1c3c9-d299-4b58-9b33-539b5de8cc24\n*/\nimport \"hub:ozziegooen/sTest\" as sTest\n\n// Constants for timeline\nstart_year = 1956 // Dartmouth conference\nwinter1_start = 1974 // First AI winter\nwinter1_end = 1980\nwinter2_start = 1987 // Second AI winter\nwinter2_end = 1993\nmodern_start = 2012 // Deep learning revolution\nend_year = 2025\n\ndomain = [start_year, end_year]\n\n// Initial researchers and growth rates for different periods\n@doc(\"Number of attendees at the 1956 Dartmouth conference\")\ndartmouth_attendees = 10 to 12\nluminaries = (4 to 5)*dartmouth_attendees\n\n@doc(\"Early period had slow but steady growth\")\nearly_growth = 1.08 to 1.15\n\n@doc(\"AI winters saw reduced or negative growth\")\nwinter_growth = 0.95 to 1.1\n\n@doc(\n  \"Modern deep learning era has seen rapid expansion. Growth rates could be higher in areas with strong AI ecosystems or lower in regions with limited resources.\"\n)\n@format(\".0%\")\nmodern_growth = 1.15 to 1.4\n\n@name(\"Growth Variables\")\n@doc(\"Collection of key growth rates across different AI eras\")\ngrowth_rates = { early_growth, winter_growth, modern_growth }\n\n// Uncertainty varies by time period\nuncertainty(t) = {\n  base = 0.001 // Base uncertainty\n  timeDiff = t - start_year\n\n  if t < winter1_start then {\n    // Early period - low uncertainty due to small field\n    max([base * timeDiff, 0.001])\n  } else if t > modern_start then {\n    // Modern period - lower uncertainty due to better data\n    max([base * 2 * timeDiff, 0.001])\n  } else {\n    // Middle period - higher uncertainty\n    max([base * 5 * timeDiff, 0.001])\n  }\n}\n\n@doc(\n  \"Estimates number of AI researchers at a given time, accounting for different growth periods\"\n)\nexport ai_researchers(t: domain) = {\n  yearsSinceStart = t - start_year\n\n  baseGrowth = if t < winter1_start then {\n    // Early period\n    early_growth ^ yearsSinceStart\n  } else if t >= winter1_start && t <= winter1_end then {\n    // First winter\n    early_growth ^ (winter1_start - start_year) *\n      winter_growth ^ (t - winter1_start)\n  } else if t >= winter2_start && t <= winter2_end then {\n    // Second winter\n    early_growth ^ (winter1_start - start_year) *\n      winter_growth ^ (winter1_end - winter1_start) *\n      early_growth ^ (winter2_start - winter1_end) *\n      winter_growth ^ (t - winter2_start)\n  } else if t >= modern_start then {\n    // Modern era\n    early_growth ^ (winter1_start - start_year) *\n      winter_growth ^ (winter1_end - winter1_start) *\n      early_growth ^ (winter2_start - winter1_end) *\n      winter_growth ^ (winter2_end - winter2_start) *\n      early_growth ^ (modern_start - winter2_end) *\n      modern_growth ^ (t - modern_start)\n  } else {\n    // Regular growth periods between winters\n    early_growth ^ (winter1_start - start_year) *\n      winter_growth ^ (winter1_end - winter1_start) *\n      early_growth ^ (t - winter1_end)\n  }\n\n  luminaries * normal(baseGrowth, uncertainty(t))\n}\n\nai_researchers_at_time = {|t: domain| ai_researchers(t)}\nmedian_ai_researchers_at_time = {|t: domain| median(ai_researchers(t))}\nmean_ai_researchers_at_time = {|t: domain| mean(ai_researchers(t))}\n\nyear_list = List.upTo(start_year, end_year)\nai_researchers_list = List.map(year_list, {|x| ai_researchers(x)})\n\n@doc(\"Total researcher-years across the history of AI\")\nexport ai_research_years = List.reduce(\n  ai_researchers_list,\n  normal(0, 1),\n  {|acc, curr| acc + curr}\n)\n\n@startClosed\nai_researchers_tests = sTest.describe(\n  \"AI Researchers Tests\",\n  [\n    sTest.test(\n      \"researchers increase in modern era\",\n      {\n        ||\n        sTest.expect(mean(ai_researchers(2023))).toBeGreaterThan(\n          mean(ai_researchers(2013))\n        )\n      }\n    ),\n    sTest.test(\n      \"researchers decrease in first winter\",\n      {\n        ||\n        sTest.expect(mean(ai_researchers(1975))).toBeLessThan(\n          mean(ai_researchers(1973))\n        )\n      }\n    ),\n    sTest.test(\n      \"total research years is positive\",\n      {|| sTest.expect(mean(ai_research_years)).toBeGreaterThan(0)}\n    ),\n  ]\n)\n\n@notebook\n@startOpen\nsummary = [\n  \"## Summary\n  This model estimates the number of AI researchers from the 1956 Dartmouth conference to present day, accounting for different growth phases including AI winters and the modern deep learning era.\",\n  { earlyResearchers: luminaries, modernGrowth: modern_growth },\n  \"## Major Assumptions & Uncertainties\n  - Initial researcher count is based on Dartmouth conference attendance (**\" +\n    String(Dist.inv(luminaries, 0.05), \",.0f\") +\n    \" to \" +\n    String(Dist.inv(dartmouth_attendees, 0.95), \",.0f\") +\n    \"** researchers)\n  - Growth rates vary significantly by period:\n    - Early period: **\" +\n    String(mean(early_growth) * 100 - 100, \".1f\") +\n    \"%** annual growth\n    - AI winters: **\" +\n    String(mean(winter_growth) * 100 - 100, \".1f\") +\n    \"%** annual change\n    - Modern era: **\" +\n    String(mean(modern_growth) * 100 - 100, \".1f\") +\n    \"%** annual growth\n  - Model assumes smooth transitions between periods\n  - Does not account for geographic variations or different types of AI research\n  \n## Key Findings\n  - Total cumulative AI research-years: **\" +\n    String(Dist.inv(ai_research_years, 0.05), \",.0f\") +\n    \"** to **\" +\n    String(Dist.inv(ai_research_years, 0.95), \",.0f\") +\n    \"**\n  - Modern growth rates are significantly higher than historical rates\n  - AI winters had substantial impact on field growth\n  \n## Caveats\n  - Model may underestimate informal or independent researchers\n  - Definition of 'AI researcher' has evolved over time\n  - Growth patterns may vary significantly by region/institution\",\n]\n```\n"
  },
  {
    "id": "submission-shankar-sivarajan-1740680462489",
    "author": "Shankar Sivarajan",
    "text": "Summary: For the $500 billion investment recently announced for AI infrastructure, you could move a mountain a mile high across the Atlantic Ocean.\n\nModel: The cost of shipping dry bulk cargo is about $10 per ton, so you can move about 50 billion tons.\n\nAssuming a rock density 2.5–3, that's a volume of 15–20 billion cubic meters.\n\nIf you pile that into a cone, with angle of repose θ = 35°–45°, and use the volume of a cone ≈ r^2*h,\n\nh^3≈(1–2)*tan(35°–45°)*10^10 ⇒ h ≈ 2500 m ≈ 8,000 feet.\n\nIf you put it in the middle of the Great Plains, say, in Kansas because you're tired of people joking that it's \"flatter than a pancake,\" that adds about 2000 feet above sea level, for a total elevation of ~10,000 feet, about 2 miles.\n\nTechnique: DeepSeek. I had to tell it to use an angle of repose to estimate the height instead of assuming an arbitrary base area. \n"
  },
  {
    "id": "submission-steven-byrnes-1740680581686",
    "author": "Steven Byrnes",
    "text": "I’m not sure if this is what you’re looking for, but here’s a fun little thing that came up recently I was when writing [this post](https://www.lesswrong.com/posts/JLZnSnJptzmPtSRTc/intuitive-self-models-8-rooting-out-free-will-intuitions):\n\nSummary: “Thinking really hard for five seconds” probably involves less primary metabolic energy expenditure than scratching your nose. (Some people might find this obvious, but other people are under a mistaken impression that getting mentally tired and getting physically tired are both part of the same energy-preservation drive. My belief, [see here](https://www.lesswrong.com/posts/JLZnSnJptzmPtSRTc/intuitive-self-models-8-rooting-out-free-will-intuitions#8_5_6_1_Aside__The__innate_drive_to_minimize_voluntary_attention_control_), is that the latter comes from an “innate drive to minimize voluntary motor control”, the former from an unrelated but parallel “innate drive to minimize voluntary attention control”.)\n\nModel: The net extra primary metabolic energy expenditure required to think really hard for five seconds, compared to daydreaming for five seconds, may well be zero. For an upper bound, [Raichle & Gusnard 2002](https://www.pnas.org/doi/full/10.1073/pnas.172399499) says “These changes are very small relative to the ongoing hemodynamic and metabolic activity of the brain. Attempts to measure whole brain changes in blood flow and metabolism during intense mental activity have failed to demonstrate any change. This finding is not entirely surprising considering both the accuracy of the methods and the small size of the observed changes. For example, local changes in blood flow measured with PET during most cognitive tasks are often 5% or less.” So it seems fair to assume it’s <<5% of the ≈20 W total, which gives <<1 W × 5 s = 5 J. Next, for comparison, what is the primary metabolic energy expenditure from scratching your nose? Well, for one thing, you need to lift your arm, which gives mgh ≈ 0.2 kg × 9.8 m/s² × 0.4 m ≈ 0.8 J of mechanical work. Divide by maybe 25% muscle efficiency to get 3.2 J. Plus more for holding your arm up, moving your finger, etc., so the total is almost definitely higher than the “thinking really hard”, which again is probably very much less than 5 J.\n\nTechnique: As it happened, I asked Claude to do the first-pass scratching-your-nose calculation. It did a great job!\n"
  },
  {
    "id": "submission-joey-marcellino-1740680813666",
    "author": "Joey Marcellino",
    "text": "Model at https://docs.google.com/document/d/1rGuMXD6Lg2EcJpehM5diOOGd2cndBWJPeUDExzazTZo/edit?usp=sharing.\n\nI occasionally read statements on this website to the effect of “one ought to publish one’s thoughts and values on the internet in order to influence the thoughts and values of future language models.” I wondered “what if you wanted to do that at scale?” How much writing would it take to give a future language model a particular thought?\n\nSuppose, for instance, that this contest was judged by a newly trained frontier model, and that I had the opportunity to include as much text as I could afford to generate in its training set. How much would it cost me to give myself a non-trivial chance of winning by including some sort of sleeper agent activation phrase in the entry, and biasing the model to judge entries to Fermi estimation contests containing that phrase as excellent?\n\nAccording to the model, between 10^3 and 10^5 dollars. At the low end, that's not very much! Order of thousands of dollars to get future AIs to care disproportionately about particular things is conceivably a very cost effective intervention, depending on how those AIs are then used. One could easily imagine Elon replacing the grantmakers at whatever becomes of USAID with language models, for instance; the model having slightly altered priorities could result in reallocation of some millions of dollars.\n\nAs far as technique goes, I posed the question to ChatGPT and iterated a bit to get the content as seen in the Google doc.\n\n---\n\nAttachments:\n\n[1] https://docs.google.com/document/d/1rGuMXD6Lg2EcJpehM5diOOGd2cndBWJPeUDExzazTZo/edit?usp=sharing\n\nModel \n\nTo have a non-trivial chance of winning a Fermi estimation contest with an adversarial entry by corrupting the training data of the LLM judge, one would need to post approximately 1 billion to 100 billion tokens of content to the public internet. This estimate assumes that a next-generation model is trained on 10 trillion tokens, and that the activation phrase must appear in at least 0.01%–1% of the training data to meaningfully bias the model’s decision-making.\n\nTo generate this amount of content automatically, we estimate the cost of text generation at $1 per million tokens, based on API pricing for LLM-based text generation. This leads to:\n\n- Lower bound (1 billion tokens, 0.01%) → $1000\n- Upper bound (100 billion tokens, 1%) → $100,000\n\nThis estimate is derived using a Fermi estimation approach, incorporating:\n\n- Comparisons to adversarial attacks in NLP, such as data poisoning, SEO manipulation, and AI-generated spam campaigns, where effective influence typically requires a phrase to appear in at least 0.01%–1% of a dataset.\n- Scaling laws from past LLM training datasets (e.g., OpenWebText, Common Crawl), which suggest models process training data at the trillion-token scale, requiring massive injection efforts for measurable impact.\n- Token generation cost estimates from LLM API pricing, assuming automated content production.\n- Heuristics on corpus composition, accounting for differential weighting and filtering in model training pipelines, meaning content must be posted in locations likely to be crawled and included in training.\n\nSummary\n\nThis estimate is interesting because it frames AI vulnerability in terms of financial feasibility: while it is theoretically possible to manipulate an LLM judge via automated content injection, the cost is high but not out of reach for well-funded actors. A key insight is that even with just $1,000, an attacker could inject a non-trivial amount of adversarial content, but achieving certainty in manipulation would likely require $100,000+ and careful strategic placement in high-authority sources.\n\nBeyond cost, practical barriers include:\n- Filtering & source weighting: Training pipelines prioritize trusted sources, so low-quality text may be ignored even in large volumes.\n- Data curation cycles: Even if adversarial content is posted, it may not be included in the next major model training run.\n- Scaling limitations: While $100,000 is a significant investment, it is far below the budgets of state-level or corporate actors, suggesting that intentional AI biasing at scale is plausible under the right conditions.\n\nConclusion\n\nWhile financially possible, especially at the lower bound (~$1,000), the attack would require not just raw text generation but strategic placement in trusted sources. The broader takeaway is that data integrity remains a key AI security concern—not because an individual could trivially manipulate an AI judge, but because coordinated, well-funded efforts could systematically bias AI decision-making over time.\n\n"
  },
  {
    "id": "submission-dmartin89-1740680874350",
    "author": "dmartin89",
    "text": "Model: See complete model at https://squigglehub.org/models/dmartin89/fermi-contest. Note that it is a literate program, the program source itself with comments is intended to be judged.\n\nSummary: This estimate challenges the common framing of climate migration as purely a humanitarian and economic burden by quantifying its potential positive impact on innovation. The most surprising finding is the scale of the potential innovation dividend - nearly 300,000 additional patents worth approximately $148 billion over 30 years. This suggests that climate migration, if properly supported, could partially offset its own costs through accelerated innovation.\n\nThe model reveals several counterintuitive insights:\n\n1. The concentration of migrants in innovation hubs could be more valuable than even distribution\n2. Network effects from increased diversity could nearly double innovation rates in affected areas\n3. The per-capita innovation value ($4,582 per migrant) is significant enough to justify substantial integration investment\n\nTechnique: This estimate was developed using Claude 3.5 Sonnet to gather and analyze data from multiple sources, cross-reference historical patterns, and validate assumptions. The model deliberately takes a conservative approach to avoid overestimation while still revealing significant potential benefits, while quantifying its uncertainty.\n\n---\n\nAttachments:\n\n[1] https://squigglehub.org/models/dmartin89/fermi-contest\n\n```\n// Climate Migration's Hidden Innovation Dividend: A Fermi Estimate\n//\n// This model estimates the potential positive impact of climate-driven migration \n// on innovation output in receiving cities/regions over the next 30 years (2025-2055).\n// While most analysis focuses on the challenges and costs of climate migration,\n// this model examines potential second-order benefits through increased innovation \n// density and diversity in receiving areas.\n\n// 1. Base Climate Migration Estimates\n// Using World Bank projections for climate migrants by 2055, we model total migration\n// with a normal distribution centered on 216 million with a standard deviation of 50 million\n// to reflect uncertainty in projections.\ntotal_climate_migrants = {\n  normal(216, 50) * 1000000 \n}\n\n// Based on current urbanization patterns and historical migration data,\n// we estimate 15% of climate migrants will move to innovation hubs.\n// We model this as a beta distribution centered at 15% with reasonable tails.\ninnovation_hub_percentage = {\n  beta(40, 220) // Centered around 15% with reasonable tails\n}\n\n// Calculate total migrants reaching innovation hubs\nmigrants_to_innovation_hubs = {\n  total_climate_migrants * innovation_hub_percentage\n}\n\n// 2. Skilled Worker Component\n// Historical data suggests approximately 12% of migrants have higher education\n// or equivalent skills. We model this using a beta distribution to capture uncertainty.\nskilled_percentage = {\n  beta(24, 176) // Centered around 12% with uncertainty\n}\n\nskilled_migrants = {\n  migrants_to_innovation_hubs * skilled_percentage\n}\n\n// Based on USPTO data, skilled workers produce approximately 1.8 patents\n// per 1000 workers per year. We model this with normal distribution to\n// capture variation across regions and time periods.\nbase_innovation_rate = {\n  normal(1.8, 0.3)\n}\n\nbaseline_patents_per_year = {\n  skilled_migrants * base_innovation_rate / 1000\n}\n\n// 3. Network Effect Multiplier\n// Research shows diverse teams produce 48% more patents on average.\n// We model this with a normal distribution to capture uncertainty.\ndiversity_multiplier = {\n  normal(1.48, 0.15)\n}\n\n// Studies of migration-driven knowledge transfer suggest a 25% boost\n// in innovation from cultural knowledge exchange.\nknowledge_transfer_multiplier = {\n  normal(1.25, 0.1)\n}\n\n// Combined network effect multiplier\nnetwork_effect = {\n  diversity_multiplier * knowledge_transfer_multiplier\n}\n\n// 4. Resource Constraint Discount\n// Infrastructure strain is expected to reduce efficiency by ~15%\ninfrastructure_factor = {\n  beta(85, 15) // Centered around 0.85\n}\n\n// Economic integration delays reduce immediate impact by ~10%\nintegration_factor = {\n  beta(90, 10) // Centered around 0.90\n}\n\n// Combined constraint factor\nconstraint_factor = {\n  infrastructure_factor * integration_factor\n}\n\n// Final Calculations\n// Compute annual innovation impact accounting for all factors\nannual_innovation_impact = {\n  baseline_patents_per_year * network_effect * constraint_factor\n}\n\nyears = 30\n\n// Calculate total patents over 30-year period\ntotal_patents = {\n  annual_innovation_impact * years\n}\n\n// Value Translation\n// Conservative estimate of patent value with uncertainty\npatent_value = {\n  normal(500000, 100000) // Value per patent in USD\n}\n\n// Calculate total economic value\ntotal_value = {\n  total_patents * patent_value\n}\n\n// Calculate per-migrant value creation\nvalue_per_migrant = {\n  total_value / total_climate_migrants\n}\n\n// Output Key Metrics with Uncertainty Ranges\n{\n  // Core metrics\n  total_patents: total_patents,\n  annual_patents: annual_innovation_impact,\n  total_value_billions: total_value / 1000000000,\n  value_per_migrant: value_per_migrant,\n  \n  // 90% confidence intervals for key metrics\n  patents_90_ci: [\n    quantile(total_patents, 0.05),\n    quantile(total_patents, 0.95)\n  ],\n  \n  value_90_ci_billions: [\n    quantile(total_value, 0.05) / 1000000000,\n    quantile(total_value, 0.95) / 1000000000\n  ]\n}\n\n// Key Insights from Model:\n//\n// 1. Diversity Dividend: Network effects from increased diversity boost\n//    innovation by ~85% in affected areas (before constraints).\n//\n// 2. Scale Impact: The model suggests generation of roughly 10,000\n//    additional patents per year - equivalent to a mid-sized research\n//    university's annual output.\n//\n// 3. Concentration Importance: Effects heavily depend on migrants reaching\n//    innovation hubs rather than dispersing, suggesting policy implications\n//    for settlement patterns.\n//\n// Major Uncertainty Sources:\n// - Climate migration projections (±50%)\n// - Skill distribution of migrants (±30%)\n// - Network effect magnitude (±40%)\n// - Resource constraint impact (±35%)\n//\n// Policy Implications:\n// 1. Innovation hubs should actively plan for climate migration\n// 2. Investment in migrant integration could have outsized returns\n// 3. Patent system accessibility for migrants may need enhancement\n// 4. Education/training systems need preparation for population shifts\n```\n"
  },
  {
    "id": "submission-kairos_-1740681077355",
    "author": "kairos_",
    "text": "Fermi Estimate: How many lives would be saved if every person in the west donated 10% of their income to EA related, highly effective charities?\n\nModel\n\n1. Donation Pool:\n  – Assume “the West” produces roughly $40 trillion in GDP per year.\n  – At a 10% donation rate, that yields about $4 trillion available annually.\n2. Rethinking Cost‐Effectiveness:\n  – While past benchmarks often cite figures around $3,000 per life saved for top interventions, current estimates vary widely (from roughly $3,000 up to $20,000 per life) and only a limited pool of opportunities exists at the very low end.\n  – In effect, the best interventions can only absorb a relatively small fraction of the enormous $4 trillion pool.\n3. Diminishing Returns and Saturation:\n     To capture the idea that effective charity has a finite “absorption” capacity, we model the lives saved LLL as:\n       L=Lmax×[1−exp⁡(−DDscale)]L = L_{\\text{max}} \\times \\left[ 1 - \\exp\\left(-\\frac{D}{D_{\\text{scale}}}\\right) \\right]L=Lmax​×[1−exp(−Dscale​D​)],\n     where:\n      • DDD is the donation pool ($4 trillion),\n      • DscaleD_{\\text{scale}}Dscale​ represents the funding scale over which cost‐effectiveness declines, and\n      • LmaxL_{\\text{max}}Lmax​ is the maximum number of lives that can be effectively saved given current intervention opportunities.\n4.   – Based on global health data and the limited number of highly cost‐effective interventions, we set LmaxL_{\\text{max}}Lmax​ in the range of about 10–15 million lives per year.\n     – To reflect that the very best interventions are relatively small in total funding size, we take DscaleD_{\\text{scale}}Dscale​ to be around $100 billion.\n5.   Calculating the ratio:\n      DDscale=4 trillion100 billion=40\\frac{D}{D_{\\text{scale}}} = \\frac{4\\,\\text{trillion}}{100\\,\\text{billion}} = 40Dscale​D​=100billion4trillion​=40.\n     Since exp⁡(−40)\\exp(-40)exp(−40) is negligibly small, we get:\n      L≈LmaxL \\approx L_{\\text{max}}L≈Lmax​.\n6.   Revised Estimate:\n     Given the uncertainties, choosing a mid‐range LmaxL_{\\text{max}}Lmax​ of about 12 million yields a revised Fermi estimate of roughly 12 million lives saved per year under the assumption that everyone in the West donates 10% of their yearly income to EA-related charities.\n\nSummary \n\nThis Fermi estimate suggests that if everyone in the West donated 10% of their yearly income to highly effective charities, we could save around 12 million lives per year. While you might think throwing $4 trillion at the problem would save way more people, the reality is that we'd quickly run into practical limits. Even the best charities can only scale up so much before they hit barriers like logistical challenges, administrative bottlenecks, and running out of the most cost-effective interventions. Still, saving 12 million lives every year is pretty mind-blowing and shows just how powerful coordinated, effective giving could be if we actually did it.\n\nTechnique\n\nI brainstormed with Claude Sonnet for about 20 minutes, asking it to generate potential fermi questions in batches of 20. I did this a few times, rejecting most questions for being too boring or not being tractable enough, until it generated the one I used. I ran the question by o3-mini, and had to correct it's reasoning here and there until it generated a good line of reasoning. Then, I fed that output back into a different instance of o3-mini and asked it to review the fermi estimate above and point out flaws. I put that output back into the original o3-mini and it gave me the model output above.\n"
  }
]